{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_plot import *\n",
    "from util_feature import * \n",
    "from util_other import *\n",
    "from util_model import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/shopping_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip all the docs as csv obviously\n",
    "\n",
    "neighbourhoods = file_path+'neighbourhoods.csv'\n",
    "listings = file_path+'listings.csv'\n",
    "listings_summary = file_path+'listings_summary.csv'\n",
    "reviews = file_path+'reviews.csv'\n",
    "reviews_summary = file_path+'reviews_summary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighb = pd.read_csv(neighbourhoods, delimiter=',')\n",
    "df_list = pd.read_csv(listings, delimiter=',')\n",
    "df_list_sum = pd.read_csv(listings_summary, delimiter=',')\n",
    "df_rev = pd.read_csv(reviews, delimiter=',')\n",
    "df_rev_sum = pd.read_csv(reviews_summary, delimiter=',')\n",
    "\n",
    "list_dataframes = [df_neighb, df_list, df_list_sum, df_rev, df_rev_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  USE THIS\n",
    "colnum =\n",
    "colcat =\n",
    "coldate =\n",
    "coltext =  []\n",
    "colcat_1hot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only looking at listings.csv, to get an idea of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now work on listings_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was seen in the plotting of the price, there are huge outliers in the price (some apartments are very expensive, but there are very few of those)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_col_distribution(df_list_sum_tronc, ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_cols_with_NaNs(df_list_sum_tronc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, we can remove the columns with too many missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_col_correl_matrix(df_list_sum_tronc, df_list_sum_tronc.columns, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns_to_keep = ['id', 'host_has_profile_pic','neighbourhood_group_cleansed', \n",
    "                   'latitude', 'longitude','property_type','room_type','accommodates','bed_type', 'price', 'cleaning_fee', \n",
    "                   'security_deposit', 'extra_people', 'guests_included', 'minimum_nights',  \n",
    "                   'instant_bookable', 'is_business_travel_ready', 'cancellation_policy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_col_correl_target(df_list_sum_tronc, columns_to_keep, ['price'], nb_to_show=10, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_col_correl_target(df_list_sum_tronc, df_list_sum_tronc.columns, ['price'], nb_to_show=5, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Engineering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the distance from the appartement to the center of Berlin\n",
    "\n",
    "berlin_center = (52.5027778, 13.404166666666667)\n",
    "df_list_sum_tronc = pd_col_add_distance_to_point(df_list_sum_tronc, berlin_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_coltext_word_frequency(df_list_sum_tronc, ['amenities'], nb_to_show=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add One hot encoding for some features in the appartment from what is inside 'amenities'\n",
    "\n",
    "df_list_sum_tronc = pd_coltext_extract_tag(df_list_sum_tronc, 'Wifi', 'amenities')\n",
    "df_list_sum_tronc = pd_coltext_extract_tag(df_list_sum_tronc, 'Laptop friendly workspace', 'amenities')\n",
    "df_list_sum_tronc = pd_coltext_extract_tag(df_list_sum_tronc, 'TV', 'amenities')\n",
    "df_list_sum_tronc = pd_coltext_extract_tag(df_list_sum_tronc, 'Free street parking', 'amenities')\n",
    "df_list_sum_tronc = pd_coltext_extract_tag(df_list_sum_tronc, 'Elevator', 'amenities')\n",
    "df_list_sum_tronc = pd_coltext_extract_tag(df_list_sum_tronc, 'Family/kid friendly', 'amenities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, just drop 'amenities' col:\n",
    "#df_list_sum_tronc = pd_col_remove(df_list_sum_tronc, ['amenities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['good', 'cozy', 'quiet', 'far', 'big', 'large', 'close', 'family', 'calm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_sum_tronc = pd_coltext_tfidf(df_list_sum_tronc, 'space', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_keep = ['id', 'host_has_profile_pic', 'neighbourhood_group_cleansed', \n",
    "                   'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bed_type', 'price', 'cleaning_fee', \n",
    "                   'security_deposit', 'extra_people', 'guests_included', 'minimum_nights',  \n",
    "                   'instant_bookable', 'is_business_travel_ready', 'cancellation_policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_keep = col_keep + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 6 rows that have missing values to be able to do Logistic Regression\n",
    "df_list_sum_tronc = df_list_sum_tronc[col_keep].dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_list_sum_tronc[columns_to_keep].loc[df_list_sum_tronc[columns_to_keep].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols_with_NaNs(df_list_sum_tronc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For modelling, removing outliers\n",
    "\n",
    "df_list_sum_tronc = pd_row_drop_above_thresh(df_list_sum_tronc, 'price', thresh=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_sum_tronc = pd_df_one_hot_encode(df_list_sum_tronc,  df_list_sum_tronc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.metrics import mean_absolute_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, target, split_ratio=0.2):\n",
    "    \n",
    "    train_X, val_X, train_y, val_y = train_test_split(df,\n",
    "                                                 df[target],\n",
    "                                                 test_size = split_ratio,\n",
    "                                                 random_state=42,\n",
    "                                                 shuffle=False)\n",
    "    print('train_X shape:',train_X.shape)\n",
    "    print('val_X shape:',val_X.shape)\n",
    "\n",
    "    print('train_y shape:',train_y.shape)\n",
    "    print('val_y shape:',val_y.shape)\n",
    "    \n",
    "    return train_X, val_X, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = split_train_test(df_list_sum_tronc, 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline = Pipeline([\n",
    "#    ('std_scaler', StandardScaler())\n",
    "#])\n",
    "\n",
    "#train_X_scale = pipeline.fit_transform(train_X)\n",
    "#val_X_scale = pipeline.fit(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know if you wanted to add this, so I simply put it here, to avoid getting warning messages during training\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "model = logReg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "\n",
    "model, train_pred, val_pred = sk_model_eval(clf, train_X, train_y, val_X, val_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
